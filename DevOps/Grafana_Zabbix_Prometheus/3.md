# AWS ƒ∞zleme Stratejileri: Zabbix, Prometheus ve Grafana ile 5 Uygulamalƒ± Senaryo

Cloud altyapƒ±larƒ±nda izleme (monitoring), sistem kararlƒ±lƒ±ƒüƒ± ve operasyonel verimlilik i√ßin √∂nemli bir birle≈üendir. Bu blog yazƒ±sƒ±n da, AWS Cloud √ºzerinde, Zabbix, Prometheus ve Grafana gibi pop√ºler a√ßƒ±k kaynaklƒ± izleme ara√ßlarƒ±nƒ±n nasƒ±l birlikte veya ayrƒ± ayrƒ± kullanƒ±labileceƒüini 5 farklƒ± uygulamalƒ± senaryo ile ele alacaƒüƒ±z.

![ZGP](./img/grafanaAWS.png)

A≈üaƒüƒ±daki tabloda senaryolarƒ±n ba≈ülƒ±k ve a√ßƒ±klamalarƒ±nƒ± √∂zet olarak bulabilirsiniz:

| Senaryo No | Senaryo Adƒ±                                                                 | A√ßƒ±klama                                                                                                                                                          |
|------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1          | EC2 Kaynak ƒ∞zleme (Zabbix + Prometheus)                                     | AWS EC2 instance‚Äôlarƒ±nƒ±n CPU, RAM, disk ve aƒü kullanƒ±mƒ± Zabbix agent ve Prometheus node_exporter kullanƒ±larak izlenir. Alarmlar ve grafik paneller olu≈üturulur.  |
| 2          | CloudWatch Loglarƒ±nƒ±n Grafana ile G√∂rselle≈ütirilmesi                        | AWS CloudWatch loglarƒ± ve metrikleri Grafana‚Äôya entegre edilerek Lambda, RDS ve API Gateway gibi servislerin performansƒ± analiz edilir.                          |
| 3          | PostgreSQL ƒ∞zleme (Zabbix Agent + Template)                                 | Docker i√ßinde √ßalƒ±≈üan PostgreSQL veritabanƒ±, Zabbix‚Äôin resmi PostgreSQL ≈üablonlarƒ±yla izlenir. Disk doluluk, baƒülantƒ± sayƒ±sƒ± ve sorgu yoƒüunluƒüu takip edilir.     |
| 4          | EKS K√ºmelerinde Prometheus ile Kubernetes ƒ∞zleme                           | AWS EKS √ºzerinde √ßalƒ±≈üan Pod, Node ve Container'lar Prometheus stack ile izlenir. Kube-state-metrics ve node-exporter ile metrikler toplanƒ±r, alert kurallarƒ± eklenir. |
| 5          | SLA Takibi ve Uptime Monitoring (Zabbix + Route53 + Grafana)                | EC2 √ºzerindeki uygulamalarƒ±n HTTP servis durumu ve port kontrolleri yapƒ±lƒ±r. Route53 health check ve Zabbix HTTP item‚Äôlarƒ±yla SLA izlenir, Grafana ile raporlanƒ±r. |


Zabbix, Grafana ve prometheus √∂zelindeki yazƒ±ma [Buradan](https://medium.com/@muratasnb/zabbix-grafana-ve-prometheus-ile-i%CC%87zleme-mimarisi-docker-ortam%C4%B1nda-uygulamal%C4%B1-senaryolar-2-889dd52f6f3f) ula≈üabilirsiniz. Hadi ≈üimdi ilk senaryomuz ile ba≈ülayalƒ±m.

## üîß 1. EC2 Sunucularƒ±nƒ±n CPU, RAM ve Disk Kullanƒ±mƒ±nƒ±n ƒ∞zlenmesi (Zabbix + Prometheus)

AWS √ºzerinde sunucularƒ±n kurulmasƒ± ve yapƒ±landƒ±rƒ±lmasƒ± AWS console √ºzerinden yapƒ±labilmektedir. Ancak bu i≈ülem uzun s√ºrebilir ve hata yapƒ±labilir bu nedenle AWS √ºzerinde kurulacak olan sistemlerde hƒ±zlƒ± ve hatasƒ±z kurmak i√ßin terraform kullanƒ±labilir. Terraform kullanƒ±mƒ± hakkƒ±ndaki yazƒ±larƒ±ma [buradan](https://medium.com/@muratasnb/terrafrom-infrastructure-as-code-iac-kavram%C4%B1-a9e60b0bd7f8) ula≈üabilirsiniz.

ƒ∞lk √∂ncelikle AWS EC2 instance 'a baƒülanƒ±labilmesi amacƒ±yla kullanƒ±labilecek anatarƒ±n AWS Console √ºzerinden ``EC2 > Network & Security > Key Pairs`` yolu izlenerek ``Create key pair`` butonu ile yeni bir anahtar √ºretilebilir. A√ßƒ±lan pencere √ºzerinden anahtar √ºretilir. 

![ZGP](./img/40.png)

Bu senaryo i√ßin terraform kullanacaƒüƒ±mƒ± belirtmi≈ütim. Altyapƒ±nƒ±n olu≈üturulmasƒ± i√ßin gerekli olan dosyalar a≈üaƒüƒ±daki gibidir;

**Dizin**

```
terraform-zabbix-monitoring/
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îî‚îÄ‚îÄ user_data/
    ‚îú‚îÄ‚îÄ zabbix_prometheus.sh
    ‚îî‚îÄ‚îÄ node_exporter.sh
```

**main.tf**
```
provider "aws" {
  region                  = "us-east-1"
  shared_credentials_files = ["~/.aws/credentials"]
  profile                 = "vscode"
}
# VPC
resource "aws_vpc" "main_vpc" {
  cidr_block = "10.0.0.0/16"
}

resource "aws_subnet" "main_subnet" {
  vpc_id     = aws_vpc.main_vpc.id
  cidr_block = "10.0.1.0/24"
  map_public_ip_on_launch = true   # ‚ùó Bu satƒ±r √∂nemli
}

resource "aws_internet_gateway" "gw" {
  vpc_id = aws_vpc.main_vpc.id
}

resource "aws_route_table" "route_table" {
  vpc_id = aws_vpc.main_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.gw.id
  }
}

resource "aws_route_table_association" "route_assoc" {
  subnet_id      = aws_subnet.main_subnet.id
  route_table_id = aws_route_table.route_table.id
}

# Security Group
resource "aws_security_group" "monitoring_sg" {
  name        = "monitoring-sg"
  description = "Allow SSH, Zabbix, Prometheus, Grafana"
  vpc_id      = aws_vpc.main_vpc.id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Zabbix agent
  ingress {
    from_port   = 10050
    to_port     = 10051
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Node Exporter
  ingress {
    from_port   = 9100
    to_port     = 9100
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Node Exporter
  ingress {
    from_port   = 8080
    to_port     = 8080
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Node Exporter
  ingress {
    from_port   = 5433
    to_port     = 5433
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Prometheus
  ingress {
    from_port   = 9090
    to_port     = 9090
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Grafana
  ingress {
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Zabbix + Grafana + Prometheus Server
resource "aws_instance" "monitoring_server" {
  ami           = "ami-0fc5d935ebf8bc3bc" # Ubuntu 22.04 (Frankfurt)
  instance_type = var.instance_type
  subnet_id     = aws_subnet.main_subnet.id
  key_name      = var.key_name
  security_groups = [aws_security_group.monitoring_sg.id]

  user_data = file("user_data/zabbix_prometheus.sh")

  tags = {
    Name = "monitoring-server"
  }
}

# ƒ∞zlenecek EC2'ler
resource "aws_instance" "monitored_node" {
  count         = 2
  ami           = "ami-0fc5d935ebf8bc3bc"
  instance_type = var.instance_type
  subnet_id     = aws_subnet.main_subnet.id
  key_name      = var.key_name
  security_groups = [aws_security_group.monitoring_sg.id]

  user_data = file("user_data/node_exporter.sh")

  tags = {
    Name = "monitored-node-${count.index + 1}"
  }
}
```

**variables.tf**
```
variable "region" {
  default = "eu-central-1"
}

variable "instance_type" {
  default = "t2.micro"
}

variable "key_name" {
  default = "Key_Monitor"
}
```

**outputs.tf**
```
output "monitoring_server_public_ip" {
  value = aws_instance.monitoring_server.public_ip
}

output "monitored_nodes_ips" {
  value = [for node in aws_instance.monitored_node : node.public_ip]
}
```

**user_data/zabbix_prometheus.sh**
```
#!/bin/bash
# Zabbix
apt update && apt install -y zabbix-server-mysql zabbix-frontend-php zabbix-apache-conf zabbix-agent

# Prometheus
useradd --no-create-home --shell /bin/false prometheus
mkdir /etc/prometheus /var/lib/prometheus

cat <<EOF > /etc/prometheus/prometheus.yml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'ec2-node-exporter'
    static_configs:
      - targets: ['10.0.1.101:9100','10.0.1.102:9100']
EOF

wget https://github.com/prometheus/prometheus/releases/latest/download/prometheus-3.4.0.linux-amd64.tar.gz
tar xvf prometheus-3.4.0.linux-amd64.tar.gz
mv prometheus-3.4.0.linux-amd64/prometheus /usr/local/bin/
nohup prometheus --config.file=/etc/prometheus/prometheus.yml &

# GPG key ve repo ekle
sudo apt update
sudo apt install -y gnupg2 curl software-properties-common

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://apt.grafana.com/gpg.key | gpg --dearmor | sudo tee /etc/apt/keyrings/grafana.gpg > /dev/null

echo "deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main" | \
  sudo tee /etc/apt/sources.list.d/grafana.list

# Paket listesi g√ºncelle ve kur
sudo apt update
sudo apt install -y grafana

# Servisi ba≈ülat ve etkinle≈ütir
sudo systemctl start grafana-server
sudo systemctl enable grafana-server

sudo apt update -y
sudo apt install -y docker.io docker-compose

# Docker √ßalƒ±≈üsƒ±n
sudo systemctl enable docker
sudo systemctl start docker

# Zabbix i√ßin klas√∂r olu≈ütur
sudo mkdir -p /opt/zabbix
sudo cd /opt/zabbix

sudo cat <<EOF > docker-compose.yml
version: '3.7'

services:
  zabbix-server:
    image: zabbix/zabbix-server-pgsql:alpine-6.0-latest
    container_name: zabbix-server
    depends_on:
      - postgres
    environment:
      DB_SERVER_HOST: postgres
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
    ports:
      - "10051:10051"
    restart: unless-stopped

  zabbix-web:
    image: zabbix/zabbix-web-apache-pgsql:alpine-6.0-latest
    container_name: zabbix-web
    depends_on:
      - zabbix-server
      - postgres
    environment:
      DB_SERVER_HOST: postgres
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
      ZBX_SERVER_HOST: zabbix-server
      PHP_TZ: Europe/Istanbul
    ports:
      - "8080:8080"
    restart: unless-stopped

  zabbix-agent:
    image: zabbix/zabbix-agent:alpine-6.0-latest
    container_name: zabbix-agent
    depends_on:
      - zabbix-server
    environment:
      ZBX_SERVER_HOST: zabbix-server
      ZBX_HOSTNAME: docker-host
    volumes:
      - /:/mnt/rootfs:ro

  postgres:
    image: postgres:13
    container_name: zabbix-postgres
    environment:
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
    ports:
      - "5433:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  pg_data:
EOF

sudo docker-compose up -d

```

**user_data/node_exporter.sh**
```
#!/bin/bash
# Zabbix Agent
apt update && apt install -y zabbix-agent
systemctl start zabbix-agent
systemctl enable zabbix-agent

# Node Exporter
wget https://github.com/prometheus/node_exporter/releases/latest/download/node_exporter-1.8.0.linux-amd64.tar.gz
tar xvf node_exporter-*.tar.gz
mv node_exporter-*/node_exporter /usr/local/bin/
nohup /usr/local/bin/node_exporter &
```

Uygulamanƒ±n √ßalƒ±≈ütƒ±rƒ±lmasƒ± amacƒ±yla a≈üaƒüƒ±daki komutlar sƒ±rasƒ±yla √ßalƒ±≈ütƒ±rƒ±lƒ±r;

```
terraform init
terraform plan
terraform apply
```

Ortam kurulumunun ardƒ±ndan ilk √∂ncelikle zabbix tarafƒ±nda host olu≈üturulmasƒ± amacƒ±yla zabbix' baƒülanƒ±lƒ±r. Baƒülantƒ± i√ßin ``http://<zabbix-server-public-ip>:8080`` adresini kullanabilirsiniz. Giri≈ü i≈ülemi i√ßin ``Admin`` kullanƒ±cƒ±sƒ±nƒ±n parolasƒ± ``zabbix`` dir. Host olu≈üturma i≈ülemi ``Configuration > Hosts`` yolu √ºzerinden ``Create host`` butonuna tƒ±klanƒ±lƒ±r.  Doldurulmasƒ± gereken alanlar;

* **Hostname:** node-1
* **Agent interface IP:** EC2 node'un private IP'si
* **Groups:** Linux servers (veya yeni grup olu≈ütur)
* **Templates:** Template OS Linux by Zabbix agent se√ß

![Zabbix](./img/41.png)

Bu i≈ülemin ardƒ±ndan Tigger (Alarm) olu≈üturulmasƒ± amacƒ±yla ``Configuration > Hosts > node-1 > Triggers`` yolu √ºzerinden ``Create Trigger`` burtonuna tƒ±klanƒ±r. A√ßƒ±lan ekran √ºzerinden a≈üaƒüƒ±daki alanlar doldurulur;

* **Name:** High CPU usage on node-1
* **Expression:**
```
last(/node-1/system.cpu.util[,user])>90
```
* **Severity:** High

![Zabbix](./img/42.png)

Bu tigger sayesinde ``node-1`` i√ßin CPU kullanƒ±m %90'ƒ± ge√ßtiƒüinde alarm √ºretir.

Prometheus tarafƒ±nda yapƒ±lan konfig√ºrasyonlarƒ±n doƒüruluƒüundan emin olunmasƒ± i√ßin ``http://<zabbix-server-public-ip>:9090`` √ºzerinde kontrol edilebilir. 
![Zabbix](./img/43.png)

Bu i≈ülemin ardƒ±ndan grafana dashboard'una ``http://<zabbix-server-public-ip>:3000`` adresi √ºzerinden baƒülanƒ±lƒ±r. ``admin`` kullanƒ±cƒ±sƒ± i√ßin ``admin`` parolasƒ± kullanƒ±lƒ±r. Prometheus data sources 'i eklenmesi amacƒ±yla ``Home > Connections > Data sources`` yolu √ºzerinden arama cubuƒüuna ``Prometheus`` yazƒ±lƒ±r. A√ßƒ±lan pencere √ºzerinden URL kƒ±smƒ±na ``http://<zabbix-server-private-ip>:9090``  yazƒ±lƒ±r ve kaydedilir.
![Zabbix](./img/44.png)

Dashboard olu≈üturulmasƒ± amacƒ±yla ``Create > Dashboard > Add panel`` prometheus se√ßilir. Kullanƒ±lacak prompt;

```
100 - (avg by (instance)(rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100)
```
![Zabbix](./img/45.png)

## 2. Grafana ile AWS CloudWatch Loglarƒ±nƒ±n G√∂rselle≈ütirilmesi: Lambda, RDS ve API Gateway ƒ∞zleme Senaryosu

AWS CloudWatch loglarƒ±nƒ±n g√∂rselle≈ütirilmesi amacƒ±yla grafana kullanƒ±labilir. ilk √∂ncelikle bu senaryonun daha iyi anla≈üƒ±lmasƒ± amacƒ±yla kullanacaƒüƒ±mƒ±z kavramlarƒ± inceleyelim.

**AWS CloudWatch**, AWS kaynaklarƒ±nƒ±n ve uygulamalarƒ±n performansƒ±nƒ± izlemek, log toplamak ve alarm olu≈üturmak i√ßin kullanƒ±lan merkezi bir g√∂zlemleme (observability) hizmetidir.
* EC2, Lambda, RDS, API Gateway gibi servislerden metrik, log ve event toplar.
* Ger√ßek zamanlƒ± grafikler, uyarƒ±lar ve analiz yapƒ±labilir.

**AWS Lambda**, sunucu kurmadan kod √ßalƒ±≈ütƒ±rmanƒ± saƒülayan bir sunucusuz (serverless) hesaplama hizmetidir.Otomatik √∂l√ßeklenir, bakƒ±m gerektirmez.

**Amazon RDS**, y√∂netilen bir veritabanƒ± hizmetidir. MySQL, PostgreSQL, MariaDB, Oracle ve SQL Server gibi ili≈ükisel veritabanlarƒ±nƒ± barƒ±ndƒ±rƒ±r.

* Backup, patching, y√ºksek eri≈üilebilirlik gibi i≈ülemleri AWS otomatik yapar.
* Uygulamalar i√ßin g√ºvenli, performanslƒ± ve √∂l√ßeklenebilir veritabanƒ± √ß√∂z√ºm√ºd√ºr.

Senaryo ortamƒ±nƒ±n olu≈üturulmasƒ± i√ßin gerekli olan terraform dosyalarƒ± a≈üaƒüƒ±daki gibidir;

**Klas√∂r Yapƒ±sƒ±**
```
terraform-zabbix-monitoring-2/
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ grafana-user-data.sh
```

**main.tf**
```yml
provider "aws" {
  region                  = "us-east-1"
  shared_credentials_files = ["~/.aws/credentials"]
  profile                 = "vscode"
}

resource "aws_iam_policy" "grafana_cloudwatch_policy" {
  name   = "GrafanaCloudWatchAccess"
  policy = file("cloudwatch-policy.json")
}

resource "aws_iam_user" "grafana_user" {
  name = "grafana-cloudwatch-user"
}

resource "aws_iam_user_policy_attachment" "grafana_attach" {
  user       = aws_iam_user.grafana_user.name
  policy_arn = aws_iam_policy.grafana_cloudwatch_policy.arn
}

resource "aws_iam_access_key" "grafana_key" {
  user = aws_iam_user.grafana_user.name
}

resource "aws_security_group" "grafana_sg" {
  name        = "grafana-sg"
  description = "Allow SSH and Grafana access"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_instance" "grafana_ec2" {
  ami                         = "ami-0fc5d935ebf8bc3bc"
  instance_type               = var.instance_type
  key_name                    = var.key_name
  subnet_id                   = var.subnet_id
  vpc_security_group_ids      = [aws_security_group.grafana_sg.id]
  user_data                   = file("grafana-user-data.sh")
  associate_public_ip_address = true

  tags = {
    Name = "grafana-cloudwatch"
  }
}

```

**outputs.tf**
```yml
output "grafana_public_ip" {
  value = aws_instance.grafana_ec2.public_ip
}

output "aws_access_key_id" {
  value = aws_iam_access_key.grafana_key.id
}

output "aws_secret_access_key" {
  value     = aws_iam_access_key.grafana_key.secret
  sensitive = true
}
```

**variables.tf**
```yml
variable "region" {
  default = "eu-central-1"
}

variable "instance_type" {
  default = "t3.micro"
}

variable "key_name" {
  description = "SSH key name"
}

variable "vpc_id" {
  description = "VPC ID"
}

variable "subnet_id" {
  description = "Subnet ID"
}
```

**grafana-user-data.sh**
```yml
#!/bin/bash
apt update
apt install -y docker.io
systemctl start docker
systemctl enable docker

docker run -d --name=grafana -p 3000:3000 grafana/grafana

```

**cloudwatch-policy.json**
```yml
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "cloudwatch:DescribeAlarms",
        "cloudwatch:GetMetricData",
        "cloudwatch:GetMetricStatistics",
        "cloudwatch:ListMetrics",
        "logs:DescribeLogGroups",
        "logs:GetLogEvents",
        "logs:FilterLogEvents"
      ],
      "Resource": "*"
    }
  ]
}
```
```cloudwatch-policy.json``` dosyasƒ±, Grafana'nƒ±n AWS CloudWatch kaynaklarƒ±na eri≈ümesini saƒülayan yetkileri tanƒ±mlar.

Alt yapƒ±nƒ±n kurulmasƒ± amacƒ±yla a≈üaƒüƒ±daki komutlar sƒ±rasƒ±yla kullanƒ±lƒ±r;

```
terraform init
terraform plan
terraform apply -var="key_name=senin-key-adin" -var="vpc_id=vpc-xxxx" -var="subnet_id=subnet-xxxx"
```
**Keyname** deƒüerini AWS console √ºzerinden ``EC2 > Key pair`` √ºzerinden elde edilebilir. **vpc_id** deƒüeri ise ``VPC > Your VPCs`` √ºzerinden elde edilebilir.  **subnet_id** depeƒüeri ise ``VPC > Subnets`` √ºzerinden elde edilebilir. AWS CLI ile de a≈üaƒüƒ±daki komutlar kullanƒ±larak deƒüerler elde edilebilir.

* **key_name**
```
aws ec2 describe-key-pairs --query "KeyPairs[*].KeyName"
```

* **vpc_id**
```
aws ec2 describe-vpcs --query "Vpcs[*].VpcId"
```

* **subnet_id**
```
aws ec2 describe-subnets --query "Subnets[*].SubnetId"
```

Ortam kurulumu tamamlandƒ±ktan sonra ``http://<grafana_public_ip>:3000`` adresi √ºzerinden grafana aray√ºz√ºne default kullanƒ±cƒ± adƒ± parola ile baƒülanabilirsiniz. ƒ∞lk √∂ncelikle ``Amazon CloudWatch`` data sources eklenmesi amacƒ±yla ``Connections > Data Sources`` yolu izlenir. ``Add Data Source`` butonuna tƒ±klanƒ±r. Arama kƒ±smƒ±na ``CloudWatch`` yazƒ±lƒ±r. Ekleme i≈üleminin tamamlanmasƒ± amacƒ±yla a≈üaƒüƒ±daki deƒüerler girilir;

| Alan                  | Deƒüer                                          |
| --------------------- | ---------------------------------------------- |
| **Auth Provider**     | `Access & secret key`                          |
| **Access Key ID**     | Terraform √ßƒ±ktƒ±sƒ±ndaki `aws_access_key_id`     |
| **Secret Access Key** | Terraform √ßƒ±ktƒ±sƒ±ndaki `aws_secret_access_key` |
| **Default Region**    | `eu-central-1` (veya kendi b√∂lgen)             |

![Zabbix](./img/46.png)

**Access Key ID** ve **Secret Access Key** deƒüerlerini ``terraform.tfstate`` dosyasƒ± i√ßerisinde bulabilirsiniz.

Ekleme i≈üleminin ardƒ±ndan Dashboard ekleme i≈ülemi i√ßin ``Create > Dashboard`` yolu izlenerek ``Add new panel`` butonuna tƒ±klanƒ±lƒ±r. A√ßƒ±lan pencere √ºzerinden a≈üaƒüƒ±daki uygun deƒüerler girilir;

* Data source: ``CloudWatch``
* Region: ``us-east-1``
* Namespace: ``AWS/Lambda``
* Metric name: ``Duration``
* Stat: ``Average``
* Period: ``1m``
* Dimensions: ``FunctionName: my-lambda-function-name``

**RDS CPUUtilization** Query Ayarlarƒ±:

* Namespace: ``AWS/RDS``
* Metric name: ``CPUUtilization``
* Stat: ``Average``
* Dimensions: ``DBInstanceIdentifier: my-rds-instance-id``

* **API Gateway Latency** Query Ayarlarƒ±:

* Namespace: ``AWS/ApiGateway``
* Metric name: ``Latency``
* Stat: ``Average``
* Dimensions: ``ApiName: my-api-name``

## 3. PostgreSQL ƒ∞zleme (Zabbix Agent + Template)

Bu senaryoda temeldeki amacƒ±mƒ±z PostgreSQL veritabanƒ±nƒ±n, Zabbix Agent aracƒ±lƒ±ƒüƒ±yla Zabbix Server'a metrik g√∂ndererek izlemektir.
izlenecek olan metrikler;

* Veritabanƒ± a√ßƒ±k mƒ±?
* Disk alanƒ± doluluk durumu
* Aktif baƒülantƒ± sayƒ±sƒ±
* Sorgu yoƒüunluƒüu ve I/O

Kullanƒ±lacak olan birle≈üenler;

| Bile≈üen             | A√ßƒ±klama                                              |
| ------------------- | ----------------------------------------------------- |
| **EC2 Instance**    | PostgreSQL + Zabbix Agent √ßalƒ±≈üacak                   |
| **Docker**          | PostgreSQL servisi container i√ßinde                   |
| **Zabbix Agent**    | PostgreSQL verilerini Zabbix Server‚Äôa iletecek        |
| **Zabbix Template** | `Template DB PostgreSQL` (Zabbix resm√Æ ≈üablonu)       |
| **Terraform**       | Ortamƒ± otomatik ve tekrar √ºretilebilir ≈üekilde kurmak |

Senaryonun ger√ßekle≈ütirilmesi ve altyapƒ±nƒ±n kurulmasƒ± amacƒ±yla a≈üaƒüƒ±daki dizin yapƒ±sƒ±na g√∂re gerekli dosyalar olu≈üturulur.

**Proje Yapƒ±sƒ±**

```
terraform-zabbix-monitoring-3/
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ cloud-init.sh
```

**main.tf**
```
provider "aws" {
  region                  = "us-east-1"
  shared_credentials_files = ["~/.aws/credentials"]
  profile                 = "vscode"
}

resource "aws_security_group" "pg_monitor_sg" {
  name   = "pg-monitoring-sg"
  vpc_id = var.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 10050
    to_port     = 10050
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 8080
    to_port     = 8080
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_instance" "pg_monitor" {
  ami                         = "ami-0fc5d935ebf8bc3bc"
  instance_type               = "t3.micro"
  key_name                    = var.key_name
  subnet_id                   = var.subnet_id
  vpc_security_group_ids      = [aws_security_group.pg_monitor_sg.id]
  associate_public_ip_address = true
  user_data                   = file("cloud-init.sh")

  tags = {
    Name = "pg-monitor-instance"
  }
}

```

**variables.tf**

```
variable "key_name" {}
variable "vpc_id" {}
variable "subnet_id" {}
```

**outputs.tf**

```
output "pg_monitor_ip" {
  value = aws_instance.pg_monitor.public_ip
}

```

**cloud-init.sh**

```
#!/bin/bash
sudo apt update
sudo apt install -y docker.io docker-compose
sudo systemctl enable docker
sudo systemctl start docker

sudo mkdir -p /opt/pg-monitor
sudo cd /opt/pg-monitor

sudo cat <<EOF > docker-compose.yml
version: '3.7'

services:
  zabbix-server:
    image: zabbix/zabbix-server-pgsql:alpine-6.0-latest
    container_name: zabbix-server
    depends_on:
      - postgres
    environment:
      DB_SERVER_HOST: postgres
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
    ports:
      - "10051:10051"
    restart: unless-stopped

  zabbix-web:
    image: zabbix/zabbix-web-apache-pgsql:alpine-6.0-latest
    container_name: zabbix-web
    depends_on:
      - zabbix-server
      - postgres
    environment:
      DB_SERVER_HOST: postgres
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
      ZBX_SERVER_HOST: zabbix-server
      PHP_TZ: Europe/Istanbul
    ports:
      - "8080:8080"
    restart: unless-stopped

  zabbix-agent:
    image: zabbix/zabbix-agent:alpine-6.0-latest
    container_name: zabbix-agent
    depends_on:
      - zabbix-server
    environment:
      ZBX_SERVER_HOST: zabbix-server
      ZBX_HOSTNAME: docker-host
    volumes:
      - /:/mnt/rootfs:ro
    ports:
      - "10050:10050"

  postgres:
    image: postgres:13
    container_name: zabbix-postgres
    environment:
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  pg_data:
EOF

sudo docker-compose up -d
```

Altyapƒ± sistemi olu≈üturulduktan sonra ``http://<zpg_monitor-server>:8080/`` baƒülanƒ±lƒ±r ve ``Admin`` kullanƒ±cƒ±sƒ±na ``zabbix`` parolasƒ± ile baƒülanƒ±lƒ±r. Host'un olu≈üturulmasƒ± amacƒ±yla ``Configuration > Hosts > Create host`` yolu izlenir. Bo≈üluklar uygun deƒüerler ile doldurulur;
* Hostname: pg-host
* Interfaces: IP olarak EC2'nin √∂zel IP‚Äôsi
* Templates: Template DB PostgreSQL se√ß

![Zabbix](./img/47.png)

**Macro Tanƒ±mlamalarƒ±**

| Macro           | Value       |
| --------------- | ----------- |
| `{$PG.HOST}`     | `localhost` |
| `{$PG.DBNAME}`   | `postgres`  |
| `{$PG.USER}`     | `zabbix`   |
| `{$PG.PASSWORD}` | `zabbix_pass`   |
| `{$PG.PORT}`     | `5432`      |


![Zabbix](./img/48.png)

Yapƒ±lmƒ±≈ü olan i≈ülemlerin doƒüruluƒüunun kontrol edilmesi amacƒ±yla ``Monitoring > Hosts`` 'a gidilir ve ``pg-host`` kontrol edilir.
Eƒüer ``Availability`` kƒ±smƒ± ye≈üil ise yapƒ±lan i≈ülem doƒüru bir ≈üekilde yapƒ±lmƒ±≈ütƒ±r. Eƒüer kƒ±rmƒ±zƒ± ise hata √∂zelinde i≈ülem yapmak gerekmektedir.

![Zabbix](./img/49.png)

Bu blog yazƒ±sƒ±nda ≈üimdilik bu kadar 4. ve 5. senaryolarƒ± i√ßin ikinci bir blog yazƒ±sƒ± yazacaƒüƒ±m. G√∂r√º≈ümek √ºzere.
