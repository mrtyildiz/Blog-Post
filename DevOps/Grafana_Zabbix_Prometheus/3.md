# AWS Ä°zleme Stratejileri: Zabbix, Prometheus ve Grafana ile 5 UygulamalÄ± Senaryo

Cloud altyapÄ±larÄ±nda izleme (monitoring), sistem kararlÄ±lÄ±ÄŸÄ± ve operasyonel verimlilik iÃ§in Ã¶nemli bir birleÅŸendir. Bu blog yazÄ±sÄ±n da, AWS Cloud Ã¼zerinde, Zabbix, Prometheus ve Grafana gibi popÃ¼ler aÃ§Ä±k kaynaklÄ± izleme araÃ§larÄ±nÄ±n nasÄ±l birlikte veya ayrÄ± ayrÄ± kullanÄ±labileceÄŸini 5 farklÄ± uygulamalÄ± senaryo ile ele alacaÄŸÄ±z.

![ZGP](./img/grafanaAWS.png)

AÅŸaÄŸÄ±daki tabloda senaryolarÄ±n baÅŸlÄ±k ve aÃ§Ä±klamalarÄ±nÄ± Ã¶zet olarak bulabilirsiniz:

| Senaryo No | Senaryo AdÄ±                                                                 | AÃ§Ä±klama                                                                                                                                                          |
|------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1          | EC2 Kaynak Ä°zleme (Zabbix + Prometheus)                                     | AWS EC2 instanceâ€™larÄ±nÄ±n CPU, RAM, disk ve aÄŸ kullanÄ±mÄ± Zabbix agent ve Prometheus node_exporter kullanÄ±larak izlenir. Alarmlar ve grafik paneller oluÅŸturulur.  |
| 2          | CloudWatch LoglarÄ±nÄ±n Grafana ile GÃ¶rselleÅŸtirilmesi                        | AWS CloudWatch loglarÄ± ve metrikleri Grafanaâ€™ya entegre edilerek Lambda, RDS ve API Gateway gibi servislerin performansÄ± analiz edilir.                          |
| 3          | PostgreSQL Ä°zleme (Zabbix Agent + Template)                                 | Docker iÃ§inde Ã§alÄ±ÅŸan PostgreSQL veritabanÄ±, Zabbixâ€™in resmi PostgreSQL ÅŸablonlarÄ±yla izlenir. Disk doluluk, baÄŸlantÄ± sayÄ±sÄ± ve sorgu yoÄŸunluÄŸu takip edilir.     |
| 4          | EKS KÃ¼melerinde Prometheus ile Kubernetes Ä°zleme                           | AWS EKS Ã¼zerinde Ã§alÄ±ÅŸan Pod, Node ve Container'lar Prometheus stack ile izlenir. Kube-state-metrics ve node-exporter ile metrikler toplanÄ±r, alert kurallarÄ± eklenir. |
| 5          | SLA Takibi ve Uptime Monitoring (Zabbix + Route53 + Grafana)                | EC2 Ã¼zerindeki uygulamalarÄ±n HTTP servis durumu ve port kontrolleri yapÄ±lÄ±r. Route53 health check ve Zabbix HTTP itemâ€™larÄ±yla SLA izlenir, Grafana ile raporlanÄ±r. |


Zabbix, Grafana ve prometheus Ã¶zelindeki yazÄ±ma [Buradan](https://medium.com/@muratasnb/zabbix-grafana-ve-prometheus-ile-i%CC%87zleme-mimarisi-docker-ortam%C4%B1nda-uygulamal%C4%B1-senaryolar-2-889dd52f6f3f) ulaÅŸabilirsiniz. Hadi ÅŸimdi ilk senaryomuz ile baÅŸlayalÄ±m.

## ğŸ”§ 1. EC2 SunucularÄ±nÄ±n CPU, RAM ve Disk KullanÄ±mÄ±nÄ±n Ä°zlenmesi (Zabbix + Prometheus)

AWS Ã¼zerinde sunucularÄ±n kurulmasÄ± ve yapÄ±landÄ±rÄ±lmasÄ± AWS console Ã¼zerinden yapÄ±labilmektedir. Ancak bu iÅŸlem uzun sÃ¼rebilir ve hata yapÄ±labilir bu nedenle AWS Ã¼zerinde kurulacak olan sistemlerde hÄ±zlÄ± ve hatasÄ±z kurmak iÃ§in terraform kullanÄ±labilir. Terraform kullanÄ±mÄ± hakkÄ±ndaki yazÄ±larÄ±ma [buradan](https://medium.com/@muratasnb/terrafrom-infrastructure-as-code-iac-kavram%C4%B1-a9e60b0bd7f8) ulaÅŸabilirsiniz.

Ä°lk Ã¶ncelikle AWS EC2 instance 'a baÄŸlanÄ±labilmesi amacÄ±yla kullanÄ±labilecek anatarÄ±n AWS Console Ã¼zerinden ``EC2 > Network & Security > Key Pairs`` yolu izlenerek ``Create key pair`` butonu ile yeni bir anahtar Ã¼retilebilir. AÃ§Ä±lan pencere Ã¼zerinden anahtar Ã¼retilir. 

![ZGP](./img/40.png)

Bu senaryo iÃ§in terraform kullanacaÄŸÄ±mÄ± belirtmiÅŸtim. AltyapÄ±nÄ±n oluÅŸturulmasÄ± iÃ§in gerekli olan dosyalar aÅŸaÄŸÄ±daki gibidir;

**Dizin**

```
terraform-zabbix-monitoring/
â”œâ”€â”€ main.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ outputs.tf
â””â”€â”€ user_data/
    â”œâ”€â”€ zabbix_prometheus.sh
    â””â”€â”€ node_exporter.sh
```

**main.tf**
```
provider "aws" {
  region                  = "us-east-1"
  shared_credentials_files = ["~/.aws/credentials"]
  profile                 = "vscode"
}
# VPC
resource "aws_vpc" "main_vpc" {
  cidr_block = "10.0.0.0/16"
}

resource "aws_subnet" "main_subnet" {
  vpc_id     = aws_vpc.main_vpc.id
  cidr_block = "10.0.1.0/24"
  map_public_ip_on_launch = true   # â— Bu satÄ±r Ã¶nemli
}

resource "aws_internet_gateway" "gw" {
  vpc_id = aws_vpc.main_vpc.id
}

resource "aws_route_table" "route_table" {
  vpc_id = aws_vpc.main_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.gw.id
  }
}

resource "aws_route_table_association" "route_assoc" {
  subnet_id      = aws_subnet.main_subnet.id
  route_table_id = aws_route_table.route_table.id
}

# Security Group
resource "aws_security_group" "monitoring_sg" {
  name        = "monitoring-sg"
  description = "Allow SSH, Zabbix, Prometheus, Grafana"
  vpc_id      = aws_vpc.main_vpc.id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Zabbix agent
  ingress {
    from_port   = 10050
    to_port     = 10051
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Node Exporter
  ingress {
    from_port   = 9100
    to_port     = 9100
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Node Exporter
  ingress {
    from_port   = 8080
    to_port     = 8080
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Node Exporter
  ingress {
    from_port   = 5433
    to_port     = 5433
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Prometheus
  ingress {
    from_port   = 9090
    to_port     = 9090
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Grafana
  ingress {
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Zabbix + Grafana + Prometheus Server
resource "aws_instance" "monitoring_server" {
  ami           = "ami-0fc5d935ebf8bc3bc" # Ubuntu 22.04 (Frankfurt)
  instance_type = var.instance_type
  subnet_id     = aws_subnet.main_subnet.id
  key_name      = var.key_name
  security_groups = [aws_security_group.monitoring_sg.id]

  user_data = file("user_data/zabbix_prometheus.sh")

  tags = {
    Name = "monitoring-server"
  }
}

# Ä°zlenecek EC2'ler
resource "aws_instance" "monitored_node" {
  count         = 2
  ami           = "ami-0fc5d935ebf8bc3bc"
  instance_type = var.instance_type
  subnet_id     = aws_subnet.main_subnet.id
  key_name      = var.key_name
  security_groups = [aws_security_group.monitoring_sg.id]

  user_data = file("user_data/node_exporter.sh")

  tags = {
    Name = "monitored-node-${count.index + 1}"
  }
}
```

**variables.tf**
```
variable "region" {
  default = "eu-central-1"
}

variable "instance_type" {
  default = "t2.micro"
}

variable "key_name" {
  default = "Key_Monitor"
}
```

**outputs.tf**
```
output "monitoring_server_public_ip" {
  value = aws_instance.monitoring_server.public_ip
}

output "monitored_nodes_ips" {
  value = [for node in aws_instance.monitored_node : node.public_ip]
}
```

**user_data/zabbix_prometheus.sh**
```
#!/bin/bash
# Zabbix
apt update && apt install -y zabbix-server-mysql zabbix-frontend-php zabbix-apache-conf zabbix-agent

# Prometheus
useradd --no-create-home --shell /bin/false prometheus
mkdir /etc/prometheus /var/lib/prometheus

cat <<EOF > /etc/prometheus/prometheus.yml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'ec2-node-exporter'
    static_configs:
      - targets: ['10.0.1.101:9100','10.0.1.102:9100']
EOF

wget https://github.com/prometheus/prometheus/releases/latest/download/prometheus-3.4.0.linux-amd64.tar.gz
tar xvf prometheus-3.4.0.linux-amd64.tar.gz
mv prometheus-3.4.0.linux-amd64/prometheus /usr/local/bin/
nohup prometheus --config.file=/etc/prometheus/prometheus.yml &

# GPG key ve repo ekle
sudo apt update
sudo apt install -y gnupg2 curl software-properties-common

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://apt.grafana.com/gpg.key | gpg --dearmor | sudo tee /etc/apt/keyrings/grafana.gpg > /dev/null

echo "deb [signed-by=/etc/apt/keyrings/grafana.gpg] https://apt.grafana.com stable main" | \
  sudo tee /etc/apt/sources.list.d/grafana.list

# Paket listesi gÃ¼ncelle ve kur
sudo apt update
sudo apt install -y grafana

# Servisi baÅŸlat ve etkinleÅŸtir
sudo systemctl start grafana-server
sudo systemctl enable grafana-server

sudo apt update -y
sudo apt install -y docker.io docker-compose

# Docker Ã§alÄ±ÅŸsÄ±n
sudo systemctl enable docker
sudo systemctl start docker

# Zabbix iÃ§in klasÃ¶r oluÅŸtur
sudo mkdir -p /opt/zabbix
sudo cd /opt/zabbix

sudo cat <<EOF > docker-compose.yml
version: '3.7'

services:
  zabbix-server:
    image: zabbix/zabbix-server-pgsql:alpine-6.0-latest
    container_name: zabbix-server
    depends_on:
      - postgres
    environment:
      DB_SERVER_HOST: postgres
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
    ports:
      - "10051:10051"
    restart: unless-stopped

  zabbix-web:
    image: zabbix/zabbix-web-apache-pgsql:alpine-6.0-latest
    container_name: zabbix-web
    depends_on:
      - zabbix-server
      - postgres
    environment:
      DB_SERVER_HOST: postgres
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
      ZBX_SERVER_HOST: zabbix-server
      PHP_TZ: Europe/Istanbul
    ports:
      - "8080:8080"
    restart: unless-stopped

  zabbix-agent:
    image: zabbix/zabbix-agent:alpine-6.0-latest
    container_name: zabbix-agent
    depends_on:
      - zabbix-server
    environment:
      ZBX_SERVER_HOST: zabbix-server
      ZBX_HOSTNAME: docker-host
    volumes:
      - /:/mnt/rootfs:ro

  postgres:
    image: postgres:13
    container_name: zabbix-postgres
    environment:
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
    ports:
      - "5433:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  pg_data:
EOF

sudo docker-compose up -d

```

**user_data/node_exporter.sh**
```
#!/bin/bash
# Zabbix Agent
apt update && apt install -y zabbix-agent
systemctl start zabbix-agent
systemctl enable zabbix-agent

# Node Exporter
wget https://github.com/prometheus/node_exporter/releases/latest/download/node_exporter-1.8.0.linux-amd64.tar.gz
tar xvf node_exporter-*.tar.gz
mv node_exporter-*/node_exporter /usr/local/bin/
nohup /usr/local/bin/node_exporter &
```

UygulamanÄ±n Ã§alÄ±ÅŸtÄ±rÄ±lmasÄ± amacÄ±yla aÅŸaÄŸÄ±daki komutlar sÄ±rasÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r;

```
terraform init
terraform plan
terraform apply
```

Ortam kurulumunun ardÄ±ndan ilk Ã¶ncelikle zabbix tarafÄ±nda host oluÅŸturulmasÄ± amacÄ±yla zabbix' baÄŸlanÄ±lÄ±r. BaÄŸlantÄ± iÃ§in ``http://<zabbix-server-public-ip>:8080`` adresini kullanabilirsiniz. GiriÅŸ iÅŸlemi iÃ§in ``Admin`` kullanÄ±cÄ±sÄ±nÄ±n parolasÄ± ``zabbix`` dir. Host oluÅŸturma iÅŸlemi ``Configuration > Hosts`` yolu Ã¼zerinden ``Create host`` butonuna tÄ±klanÄ±lÄ±r.  DoldurulmasÄ± gereken alanlar;

* **Hostname:** node-1
* **Agent interface IP:** EC2 node'un private IP'si
* **Groups:** Linux servers (veya yeni grup oluÅŸtur)
* **Templates:** Template OS Linux by Zabbix agent seÃ§

![Zabbix](./img/41.png)

Bu iÅŸlemin ardÄ±ndan Tigger (Alarm) oluÅŸturulmasÄ± amacÄ±yla ``Configuration > Hosts > node-1 > Triggers`` yolu Ã¼zerinden ``Create Trigger`` burtonuna tÄ±klanÄ±r. AÃ§Ä±lan ekran Ã¼zerinden aÅŸaÄŸÄ±daki alanlar doldurulur;

* **Name:** High CPU usage on node-1
* **Expression:**
```
last(/node-1/system.cpu.util[,user])>90
```
* **Severity:** High

![Zabbix](./img/42.png)

Bu tigger sayesinde ``node-1`` iÃ§in CPU kullanÄ±m %90'Ä± geÃ§tiÄŸinde alarm Ã¼retir.

Prometheus tarafÄ±nda yapÄ±lan konfigÃ¼rasyonlarÄ±n doÄŸruluÄŸundan emin olunmasÄ± iÃ§in ``http://<zabbix-server-public-ip>:9090`` Ã¼zerinde kontrol edilebilir. 
![Zabbix](./img/43.png)

Bu iÅŸlemin ardÄ±ndan grafana dashboard'una ``http://<zabbix-server-public-ip>:3000`` adresi Ã¼zerinden baÄŸlanÄ±lÄ±r. ``admin`` kullanÄ±cÄ±sÄ± iÃ§in ``admin`` parolasÄ± kullanÄ±lÄ±r. Prometheus data sources 'i eklenmesi amacÄ±yla ``Home > Connections > Data sources`` yolu Ã¼zerinden arama cubuÄŸuna ``Prometheus`` yazÄ±lÄ±r. AÃ§Ä±lan pencere Ã¼zerinden URL kÄ±smÄ±na ``http://<zabbix-server-private-ip>:9090``  yazÄ±lÄ±r ve kaydedilir.
![Zabbix](./img/44.png)

Dashboard oluÅŸturulmasÄ± amacÄ±yla ``Create > Dashboard > Add panel`` prometheus seÃ§ilir. KullanÄ±lacak prompt;

```
100 - (avg by (instance)(rate(node_cpu_seconds_total{mode="idle"}[1m])) * 100)
```
![Zabbix](./img/45.png)

## 2. Grafana ile AWS CloudWatch LoglarÄ±nÄ±n GÃ¶rselleÅŸtirilmesi: Lambda, RDS ve API Gateway Ä°zleme Senaryosu

AWS CloudWatch loglarÄ±nÄ±n gÃ¶rselleÅŸtirilmesi amacÄ±yla grafana kullanÄ±labilir. ilk Ã¶ncelikle bu senaryonun daha iyi anlaÅŸÄ±lmasÄ± amacÄ±yla kullanacaÄŸÄ±mÄ±z kavramlarÄ± inceleyelim.

**AWS CloudWatch**, AWS kaynaklarÄ±nÄ±n ve uygulamalarÄ±n performansÄ±nÄ± izlemek, log toplamak ve alarm oluÅŸturmak iÃ§in kullanÄ±lan merkezi bir gÃ¶zlemleme (observability) hizmetidir.
* EC2, Lambda, RDS, API Gateway gibi servislerden metrik, log ve event toplar.
* GerÃ§ek zamanlÄ± grafikler, uyarÄ±lar ve analiz yapÄ±labilir.

**AWS Lambda**, sunucu kurmadan kod Ã§alÄ±ÅŸtÄ±rmanÄ± saÄŸlayan bir sunucusuz (serverless) hesaplama hizmetidir.Otomatik Ã¶lÃ§eklenir, bakÄ±m gerektirmez.

**Amazon RDS**, yÃ¶netilen bir veritabanÄ± hizmetidir. MySQL, PostgreSQL, MariaDB, Oracle ve SQL Server gibi iliÅŸkisel veritabanlarÄ±nÄ± barÄ±ndÄ±rÄ±r.

* Backup, patching, yÃ¼ksek eriÅŸilebilirlik gibi iÅŸlemleri AWS otomatik yapar.
* Uygulamalar iÃ§in gÃ¼venli, performanslÄ± ve Ã¶lÃ§eklenebilir veritabanÄ± Ã§Ã¶zÃ¼mÃ¼dÃ¼r.

Senaryo ortamÄ±nÄ±n oluÅŸturulmasÄ± iÃ§in gerekli olan terraform dosyalarÄ± aÅŸaÄŸÄ±daki gibidir;

**KlasÃ¶r YapÄ±sÄ±**
```
terraform-zabbix-monitoring-2/
â”œâ”€â”€ main.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ outputs.tf
â”œâ”€â”€ grafana-user-data.sh
```

**main.tf**
```yml
provider "aws" {
  region                  = "us-east-1"
  shared_credentials_files = ["~/.aws/credentials"]
  profile                 = "vscode"
}

resource "aws_iam_policy" "grafana_cloudwatch_policy" {
  name   = "GrafanaCloudWatchAccess"
  policy = file("cloudwatch-policy.json")
}

resource "aws_iam_user" "grafana_user" {
  name = "grafana-cloudwatch-user"
}

resource "aws_iam_user_policy_attachment" "grafana_attach" {
  user       = aws_iam_user.grafana_user.name
  policy_arn = aws_iam_policy.grafana_cloudwatch_policy.arn
}

resource "aws_iam_access_key" "grafana_key" {
  user = aws_iam_user.grafana_user.name
}

resource "aws_security_group" "grafana_sg" {
  name        = "grafana-sg"
  description = "Allow SSH and Grafana access"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_instance" "grafana_ec2" {
  ami                         = "ami-0fc5d935ebf8bc3bc"
  instance_type               = var.instance_type
  key_name                    = var.key_name
  subnet_id                   = var.subnet_id
  vpc_security_group_ids      = [aws_security_group.grafana_sg.id]
  user_data                   = file("grafana-user-data.sh")
  associate_public_ip_address = true

  tags = {
    Name = "grafana-cloudwatch"
  }
}

```

**outputs.tf**
```yml
output "grafana_public_ip" {
  value = aws_instance.grafana_ec2.public_ip
}

output "aws_access_key_id" {
  value = aws_iam_access_key.grafana_key.id
}

output "aws_secret_access_key" {
  value     = aws_iam_access_key.grafana_key.secret
  sensitive = true
}
```

**variables.tf**
```yml
variable "region" {
  default = "eu-central-1"
}

variable "instance_type" {
  default = "t3.micro"
}

variable "key_name" {
  description = "SSH key name"
}

variable "vpc_id" {
  description = "VPC ID"
}

variable "subnet_id" {
  description = "Subnet ID"
}
```

**grafana-user-data.sh**
```yml
#!/bin/bash
apt update
apt install -y docker.io
systemctl start docker
systemctl enable docker

docker run -d --name=grafana -p 3000:3000 grafana/grafana

```

**cloudwatch-policy.json**
```yml
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "cloudwatch:DescribeAlarms",
        "cloudwatch:GetMetricData",
        "cloudwatch:GetMetricStatistics",
        "cloudwatch:ListMetrics",
        "logs:DescribeLogGroups",
        "logs:GetLogEvents",
        "logs:FilterLogEvents"
      ],
      "Resource": "*"
    }
  ]
}
```
```cloudwatch-policy.json``` dosyasÄ±, Grafana'nÄ±n AWS CloudWatch kaynaklarÄ±na eriÅŸmesini saÄŸlayan yetkileri tanÄ±mlar.

Alt yapÄ±nÄ±n kurulmasÄ± amacÄ±yla aÅŸaÄŸÄ±daki komutlar sÄ±rasÄ±yla kullanÄ±lÄ±r;

```
terraform init
terraform plan
terraform apply -var="key_name=senin-key-adin" -var="vpc_id=vpc-xxxx" -var="subnet_id=subnet-xxxx"
```
**Keyname** deÄŸerini AWS console Ã¼zerinden ``EC2 > Key pair`` Ã¼zerinden elde edilebilir. **vpc_id** deÄŸeri ise ``VPC > Your VPCs`` Ã¼zerinden elde edilebilir.  **subnet_id** depeÄŸeri ise ``VPC > Subnets`` Ã¼zerinden elde edilebilir. AWS CLI ile de aÅŸaÄŸÄ±daki komutlar kullanÄ±larak deÄŸerler elde edilebilir.

* **key_name**
```
aws ec2 describe-key-pairs --query "KeyPairs[*].KeyName"
```

* **vpc_id**
```
aws ec2 describe-vpcs --query "Vpcs[*].VpcId"
```

* **subnet_id**
```
aws ec2 describe-subnets --query "Subnets[*].SubnetId"
```

Ortam kurulumu tamamlandÄ±ktan sonra ``http://<grafana_public_ip>:3000`` adresi Ã¼zerinden grafana arayÃ¼zÃ¼ne default kullanÄ±cÄ± adÄ± parola ile baÄŸlanabilirsiniz. Ä°lk Ã¶ncelikle ``Amazon CloudWatch`` data sources eklenmesi amacÄ±yla ``Connections > Data Sources`` yolu izlenir. ``Add Data Source`` butonuna tÄ±klanÄ±r. Arama kÄ±smÄ±na ``CloudWatch`` yazÄ±lÄ±r. Ekleme iÅŸleminin tamamlanmasÄ± amacÄ±yla aÅŸaÄŸÄ±daki deÄŸerler girilir;

| Alan                  | DeÄŸer                                          |
| --------------------- | ---------------------------------------------- |
| **Auth Provider**     | `Access & secret key`                          |
| **Access Key ID**     | Terraform Ã§Ä±ktÄ±sÄ±ndaki `aws_access_key_id`     |
| **Secret Access Key** | Terraform Ã§Ä±ktÄ±sÄ±ndaki `aws_secret_access_key` |
| **Default Region**    | `eu-central-1` (veya kendi bÃ¶lgen)             |

![Zabbix](./img/46.png)

**Access Key ID** ve **Secret Access Key** deÄŸerlerini ``terraform.tfstate`` dosyasÄ± iÃ§erisinde bulabilirsiniz.

Ekleme iÅŸleminin ardÄ±ndan Dashboard ekleme iÅŸlemi iÃ§in ``Create > Dashboard`` yolu izlenerek ``Add new panel`` butonuna tÄ±klanÄ±lÄ±r. AÃ§Ä±lan pencere Ã¼zerinden aÅŸaÄŸÄ±daki uygun deÄŸerler girilir;

* Data source: ``CloudWatch``
* Region: ``us-east-1``
* Namespace: ``AWS/Lambda``
* Metric name: ``Duration``
* Stat: ``Average``
* Period: ``1m``
* Dimensions: ``FunctionName: my-lambda-function-name``

**RDS CPUUtilization** Query AyarlarÄ±:

* Namespace: ``AWS/RDS``
* Metric name: ``CPUUtilization``
* Stat: ``Average``
* Dimensions: ``DBInstanceIdentifier: my-rds-instance-id``

* **API Gateway Latency** Query AyarlarÄ±:

* Namespace: ``AWS/ApiGateway``
* Metric name: ``Latency``
* Stat: ``Average``
* Dimensions: ``ApiName: my-api-name``

## 3. PostgreSQL Ä°zleme (Zabbix Agent + Template)

Bu senaryoda temeldeki amacÄ±mÄ±z PostgreSQL veritabanÄ±nÄ±n, Zabbix Agent aracÄ±lÄ±ÄŸÄ±yla Zabbix Server'a metrik gÃ¶ndererek izlemektir.
izlenecek olan metrikler;

* VeritabanÄ± aÃ§Ä±k mÄ±?
* Disk alanÄ± doluluk durumu
* Aktif baÄŸlantÄ± sayÄ±sÄ±
* Sorgu yoÄŸunluÄŸu ve I/O

KullanÄ±lacak olan birleÅŸenler;

| BileÅŸen             | AÃ§Ä±klama                                              |
| ------------------- | ----------------------------------------------------- |
| **EC2 Instance**    | PostgreSQL + Zabbix Agent Ã§alÄ±ÅŸacak                   |
| **Docker**          | PostgreSQL servisi container iÃ§inde                   |
| **Zabbix Agent**    | PostgreSQL verilerini Zabbix Serverâ€™a iletecek        |
| **Zabbix Template** | `Template DB PostgreSQL` (Zabbix resmÃ® ÅŸablonu)       |
| **Terraform**       | OrtamÄ± otomatik ve tekrar Ã¼retilebilir ÅŸekilde kurmak |

Senaryonun gerÃ§ekleÅŸtirilmesi ve altyapÄ±nÄ±n kurulmasÄ± amacÄ±yla aÅŸaÄŸÄ±daki dizin yapÄ±sÄ±na gÃ¶re gerekli dosyalar oluÅŸturulur.

**Proje YapÄ±sÄ±**

```
terraform-zabbix-monitoring-3/
â”œâ”€â”€ main.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ outputs.tf
â”œâ”€â”€ cloud-init.sh
```

**main.tf**
```
provider "aws" {
  region                  = "us-east-1"
  shared_credentials_files = ["~/.aws/credentials"]
  profile                 = "vscode"
}

resource "aws_security_group" "pg_monitor_sg" {
  name   = "pg-monitoring-sg"
  vpc_id = var.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 10050
    to_port     = 10050
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 8080
    to_port     = 8080
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_instance" "pg_monitor" {
  ami                         = "ami-0fc5d935ebf8bc3bc"
  instance_type               = "t3.micro"
  key_name                    = var.key_name
  subnet_id                   = var.subnet_id
  vpc_security_group_ids      = [aws_security_group.pg_monitor_sg.id]
  associate_public_ip_address = true
  user_data                   = file("cloud-init.sh")

  tags = {
    Name = "pg-monitor-instance"
  }
}

```

**variables.tf**

```
variable "key_name" {}
variable "vpc_id" {}
variable "subnet_id" {}
```

**outputs.tf**

```
output "pg_monitor_ip" {
  value = aws_instance.pg_monitor.public_ip
}

```

**cloud-init.sh**

```
#!/bin/bash
sudo apt update
sudo apt install -y docker.io docker-compose
sudo systemctl enable docker
sudo systemctl start docker

sudo mkdir -p /opt/pg-monitor
sudo cd /opt/pg-monitor

sudo cat <<EOF > docker-compose.yml
version: '3.7'

services:
  zabbix-server:
    image: zabbix/zabbix-server-pgsql:alpine-6.0-latest
    container_name: zabbix-server
    depends_on:
      - postgres
    environment:
      DB_SERVER_HOST: postgres
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
    ports:
      - "10051:10051"
    restart: unless-stopped

  zabbix-web:
    image: zabbix/zabbix-web-apache-pgsql:alpine-6.0-latest
    container_name: zabbix-web
    depends_on:
      - zabbix-server
      - postgres
    environment:
      DB_SERVER_HOST: postgres
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
      ZBX_SERVER_HOST: zabbix-server
      PHP_TZ: Europe/Istanbul
    ports:
      - "8080:8080"
    restart: unless-stopped

  zabbix-agent:
    image: zabbix/zabbix-agent:alpine-6.0-latest
    container_name: zabbix-agent
    depends_on:
      - zabbix-server
    environment:
      ZBX_SERVER_HOST: zabbix-server
      ZBX_HOSTNAME: docker-host
    volumes:
      - /:/mnt/rootfs:ro
    ports:
      - "10050:10050"

  postgres:
    image: postgres:13
    container_name: zabbix-postgres
    environment:
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: zabbix_pass
      POSTGRES_DB: zabbix
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  pg_data:
EOF

sudo docker-compose up -d
```

AltyapÄ± sistemi oluÅŸturulduktan sonra ``http://<zpg_monitor-server>:8080/`` baÄŸlanÄ±lÄ±r ve ``Admin`` kullanÄ±cÄ±sÄ±na ``zabbix`` parolasÄ± ile baÄŸlanÄ±lÄ±r. Host'un oluÅŸturulmasÄ± amacÄ±yla ``Configuration > Hosts > Create host`` yolu izlenir. BoÅŸluklar uygun deÄŸerler ile doldurulur;
* Hostname: pg-host
* Interfaces: IP olarak EC2'nin Ã¶zel IPâ€™si
* Templates: Template DB PostgreSQL seÃ§

![Zabbix](./img/47.png)

**Macro TanÄ±mlamalarÄ±**

| Macro           | Value       |
| --------------- | ----------- |
| `{$PG.HOST}`     | `localhost` |
| `{$PG.DBNAME}`   | `postgres`  |
| `{$PG.USER}`     | `zabbix`   |
| `{$PG.PASSWORD}` | `zabbix_pass`   |
| `{$PG.PORT}`     | `5432`      |


![Zabbix](./img/48.png)

YapÄ±lmÄ±ÅŸ olan iÅŸlemlerin doÄŸruluÄŸunun kontrol edilmesi amacÄ±yla ``Monitoring > Hosts`` 'a gidilir ve ``pg-host`` kontrol edilir.
EÄŸer ``Availability`` kÄ±smÄ± yeÅŸil ise yapÄ±lan iÅŸlem doÄŸru bir ÅŸekilde yapÄ±lmÄ±ÅŸtÄ±r. EÄŸer kÄ±rmÄ±zÄ± ise hata Ã¶zelinde iÅŸlem yapmak gerekmektedir.

![Zabbix](./img/49.png)

Bu blog yazÄ±sÄ±nda ÅŸimdilik bu kadar 4. ve 5. senaryolarÄ± iÃ§in ikinci bir blog yazÄ±sÄ± yazacaÄŸÄ±m. GÃ¶rÃ¼ÅŸmek Ã¼zere.
